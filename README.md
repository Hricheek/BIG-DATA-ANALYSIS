**# BIG DATA ANALYSIS**

# Company:CODTECH IT SOLUTIONS

# Name:Hricheek Bhattacharjee

# Intern ID:CT04WB108

# Domain:Data Analysis

# Mentor:Neela Santosh

# Description:So, I recently worked on analyzing a large banking transaction dataset using Python, and I had to make sure the process was efficient, given the size of the dataset. Since working with large files can be tricky, I decided to use Dask, which is a great tool for parallel computing and handling big data efficiently. However, Dask does not support reading Excel files directly, which was a bit of a challenge. To overcome this, I used Pandas for reading the Excel file and then combined it with Dask’s delayed execution to process the data efficiently without consuming too much memory.
The first step was setting up everything correctly. Since Dask operates lazily, meaning it doesn’t execute operations immediately, I had to make sure the dataset would be loaded only when required. Instead of loading the entire Excel file at once, which could have caused memory issues, I used a function that reads the data using Pandas and then wrapped it inside a delayed execution method. This allowed me to control when the data was actually processed, making it much more memory-efficient.Once the data was loaded, I wanted to check if everything was working fine. The best way to do that was by displaying the first few rows of the dataset, just like we do in Pandas. This step was crucial because it confirmed that the data had been successfully read without overwhelming my system. Since I was dealing with banking transactions, the dataset had details like account numbers, deposit amounts, and withdrawal amounts.After ensuring the dataset was correctly loaded, I moved on to the analysis part. One of the first things I did was compute the average and total deposit amounts and the average and total withdrawal amounts. Dask made this process very smooth because it automatically parallelized the computations, making them much faster compared to Pandas.Another important analysis I performed was grouping transactions by account number. This allowed me to see the total deposit and withdrawal amounts for each account, which provided a clear picture of how each account was performing. This step was crucial for understanding transaction behavior at the individual account level. Again, since Dask works lazily, I had to explicitly tell it to execute the computation at this stage.
The insights gained from this analysis were quite useful. By looking at transaction patterns, I could identify potential fraudulent activities, such as accounts with abnormally high withdrawals or deposits. This is something banks often look for when trying to detect suspicious transactions. If an account suddenly had an unusually large withdrawal compared to its history, it could indicate fraud or unauthorized activity. Similarly, accounts with excessive deposits could signal money laundering or other financial irregularities.Another practical use of this analysis was in risk assessment. Businesses, especially those in finance, could use this data to flag high-risk accounts for further investigation. Instead of manually going through thousands of transactions, this kind of analysis allows for automated monitoring, saving time and effort.
Overall, working with Dask for big data analysis was a great experience. The main takeaway for me was that Dask’s lazy execution model is a game-changer for handling large datasets. Instead of loading everything into memory at once, it only processes data when needed, making it much more efficient. The ability to parallelize computations also significantly speeds up the analysis, which is crucial when working with massive datasets like banking transactions.

# output: ![Image](https://github.com/user-attachments/assets/fe413b4a-ab4f-4968-aa76-e146474b4e18)

